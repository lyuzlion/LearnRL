本章核心是**最优策略（Optimal Policy）**与**最优状态值（Optimal State Value）**，并通过**贝尔曼最优方程（Bellman Optimality Equation, BOE）** 这一关键工具进行分析。

**1. 核心概念**
- **最优策略**：若某策略在**所有状态**下的状态值均不低于任何其他策略，则该策略为最优。
- **最优状态值**：最优策略对应的状态值称为最优状态值。

**2. 贝尔曼最优方程（BOE）**
- BOE 是用于求解最优策略与最优状态值的基本方程，其元素形式为：
  \[
  v(s) = \max_{\pi(s)} \sum_a \pi(a|s) \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v(s') \right]
  \]
  其中 \(\gamma\) 为**折扣率（Discount Rate）**。
- 矩阵向量形式为：
  \[
  v = \max_{\pi} (r_\pi + \gamma P_\pi v)
  \]
  可简写为 \(v = f(v)\)，其中 \(f\) 具有**收缩映射（Contraction Mapping）** 性质。

**3. 求解与性质**
- **存在性与唯一性**：BOE 的解 \(v^*\)（最优状态值）存在且唯一；对应最优策略 \(\pi^*\) 不一定唯一。
- **求解方法**：可通过**值迭代（Value Iteration）** 算法迭代求解：
  \[
  v_{k+1} = \max_{\pi} (r_\pi + \gamma P_\pi v_k)
  \]
- **最优策略形式**：总存在一个**确定性贪心策略（Deterministic Greedy Policy）** 是最优的，即：
  \[
  \pi^*(a|s) = 
  \begin{cases} 
  1, & a = \arg\max_a q^*(s,a) \\
  0, & \text{否则}
  \end{cases}
  \]
  其中 \(q^*(s,a)\) 为**最优动作值（Optimal Action Value）**。

**4. 影响最优策略的因素**
- **折扣率 \(\gamma\)**：\(\gamma\) 越小，策略越**短视（Short-sighted）**；\(\gamma = 0\) 时只考虑即时奖励。
- **奖励值 \(r\)**：奖励的**仿射变换（Affine Transformation）**（如 \(r \to \alpha r + \beta\)）不改变最优策略，仅按比例平移最优状态值。
- **系统模型 \(p(s'|s,a)\)**：由环境决定，固定不变。

**5. 关键结论**
- BOE 是**贝尔曼方程（Bellman Equation）** 的特殊形式，对应策略为最优。
- 由于 \(f(v)\) 是收缩映射，可应用**压缩映射定理（Contraction Mapping Theorem）** 保证解的存在、唯一性与迭代收敛性。
- 最优策略不一定唯一，也可能是**随机策略（Stochastic Policy）**，但总存在确定性最优策略。
- **折扣机制**能自然避免无效绕路，无需额外设置负奖励鼓励快速到达目标。

**6. 常见问答摘要**
- **最优策略存在吗？**：一定存在。
- **唯一吗？**：不一定，可能有多个最优策略对应同一 \(v^*\)。
- **是随机还是确定？**：可以是随机，但总存在确定性最优策略。
- **如何求解？**：通过求解 BOE（如值迭代）。
- **折扣率降低的影响？**：策略更短视，不愿为长期回报冒险。
