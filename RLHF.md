# 基于人类反馈的强化学习, RLHF


### **第一阶段：监督微调**

在RLHF开始前，我们通常需要一个起点较好的模型。
1.  **起点**：我们有一个通过海量互联网文本训练出来的**基础大模型**。它知识渊博，但行为不可控，可能输出有害、冗长或不符合指令的内容。
2.  **任务**：收集一批高质量的**提示-回答对**数据。这些数据由人类专家精心撰写，展示了在特定提示下，我们期望模型输出的“理想答案”是什么样的。
3.  **训练**：用这批数据对基础模型进行**监督微调**。这个过程相当于教模型模仿人类的优秀示范。
4.  **产出**：我们得到了一个**SFT模型**。它已经能较好地遵循指令，但能力上限被示范数据所限制，且为了追求“正确”可能过于保守。


### **第二阶段：奖励模型（Reward Model）训练**

这是RLHF最核心的创新点。我们不是直接教模型，而是教一个“裁判”如何打分。
1.  **任务 - 生成比较数据**：
    *   对于同一个提示（例如：“解释一下量子计算”），让SFT模型生成**多个不同的回答**（例如：4个）。
    *   将这些回答匿名后，提交给人类标注员，让他们进行**两两比较**，判断哪个回答“更好”。这个“更好”可以综合**有用性、真实性、无害性、详细程度**等多个维度。
    *   最终，我们得到的是一个庞大的**偏好比较数据集**，形式如：（提示， 获胜回答， 失败回答）。
2.  **训练 - 学习人类的偏好**：
    *   我们初始化一个**奖励模型**，通常基于SFT模型的结构，将最后的**输出层改为一个标量**输出（即“分数”）。
    *   训练目标非常巧妙：**对于一组比较数据，让奖励模型给“获胜回答”打出的分数，显著高于给“失败回答”打出的分数**。
    *   损失函数：**配对排名损失**。\[
\mathcal{L}_{\text{ranking}}(\theta) = - \mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma\left( r_{\theta}(x, y_w) - r_{\theta}(x, y_l) \right) \right]
\]
3.  **产出**：我们得到了一个相对可靠的**奖励模型**。它可以对任何（提示， 回答）自动给出一个分数，这个分数代表了其符合人类偏好的程度。


### **第三阶段：强化学习微调**

这是让模型最终“对齐”的关键步骤。我们让模型在RM的指导下自我进化。
1.  **设置**：
    *   **智能体**：需要优化的SFT模型（我们的“策略”）。
    *   **环境**：文本生成环境。给定一个提示，模型生成一个完整的回答作为一次“行动”。
    *   **奖励**：由第二阶段训练好的**奖励模型**提供。对于生成的回答，RM给出一个总分。
    *   **挑战**：如果只追求RM分数最大化，模型可能会学会“钻空子”，生成迎合RM但毫无意义的长篇大论（奖励黑客）。
2.  **引入约束 - 防止偏离SFT模型**：
    *   为了解决上述问题，我们引入一个**重要性惩罚项**：在最终奖励中，不仅加入RM的分数，还会减去一个**KL散度惩罚项**。
    *   **KL散度**衡量当前模型输出与原始SFT模型输出的分布差异。这个惩罚项**防止当前模型为了追求高分而偏离SFT模型太远**，从而保持生成文本的语言质量和 coherence，并有效抑制“奖励黑客”行为。
    *   最终奖励函数可以简化为：`总奖励 = RM(回答) - β * KL(当前模型 || SFT模型)`，其中β是调节系数。
3.  **训练 - 近端策略优化**：
    *   这是最常用的优化算法。过程是迭代式的：
        a. 从数据集中采样一批prompt。
        b. 用当前的策略模型为每个prompt生成回答。
        c. 用奖励模型和KL惩罚计算每个回答的总奖励。
        d. 使用PPO算法更新策略模型的参数，**使生成高奖励回答的概率增加，生成低奖励回答的概率降低**。
4.  **产出**：我们得到了最终的**RLHF微调模型**。它既保持了知识能力，又在RM的引导下优化了输出风格和内容，使其更符合人类的复杂偏好。


**全过程流程图**：
`监督微调(SFT)` → `人类偏好数据采集` → `奖励模型训练(RM)` → `RL微调(PPO+KL惩罚)` → `最终的RLHF模型`

**关键优势**：
*   能够优化那些难以用简单规则描述、但人类一眼就能判断的复杂目标（如“有用且无害”）。
*   让模型的行为与复杂、多维的人类价值观对齐。




















在RLHF的第二阶段训练奖励模型时，**配对排名损失**是用于学习人类偏好的核心损失函数。其目标是使奖励模型对“更好”的回答打出更高分数，对“更差”的回答打出更低分数。最常用的公式基于**负对数似然损失**，具体如下：

### **配对排名损失的标准公式**


\[
\mathcal{L}_{\text{ranking}}(\theta) = - \mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma\left( r_{\theta}(x, y_w) - r_{\theta}(x, y_l) \right) \right]
\]
其中：
- \(\mathcal{L}_{\text{ranking}}(\theta)\)：损失函数，依赖于奖励模型的参数 \(\theta\)。
- \(\mathbb{E}_{(x, y_w, y_l) \sim D}\)：对数据集 \(D\) 中的样本求期望。数据集 \(D\) 由三元组 \((x, y_w, y_l)\) 组成。
- \(x\)：提示（输入文本）。
- \(y_w\)：获胜回答（人类标注员偏好的回答）。
- \(y_l\)：失败回答（人类标注员不偏好的回答）。
- \(r_{\theta}(x, y)\)：奖励模型对给定提示 \(x\) 和回答 \(y\) 预测的标量分数（奖励值），参数为 \(\theta\)。
- \(\sigma\)：Sigmoid函数，定义为 \(\sigma(z) = \frac{1}{1 + e^{-z}}\)，将分数差映射到 \((0, 1)\) 区间，表示获胜回答优于失败回答的概率。

### **公式的直观解释**
- **核心思想**：最大化获胜回答与失败回答的分数差 \( r_{\theta}(x, y_w) - r_{\theta}(x, y_l) \)。
- **Sigmoid 和 log 的作用**：  
  - 分数差越大，\(\sigma(\text{差})\) 越接近 1，\( \log(\sigma(\text{差})) \) 越接近 0（损失越小）。  
  - 分数差为负（即排序错误），\(\sigma(\text{差}) < 0.5\)，导致较大的负对数损失（即损失较大）。  
  - 这相当于一个平滑的排序约束，鼓励模型明确区分回答质量。

### **训练时的实际操作**
1. 对于数据集中的每个三元组 \((x, y_w, y_l)\)，奖励模型分别计算 \( r_{\theta}(x, y_w) \) 和 \( r_{\theta}(x, y_l) \)。
2. 计算分数差 \( \Delta = r_{\theta}(x, y_w) - r_{\theta}(x, y_l) \)。
3. 损失为 \( -\log \sigma(\Delta) \)。
4. 通过梯度下降最小化所有样本的平均损失，从而更新参数 \(\theta\)，使奖励模型逐渐符合人类偏好。

